---
title: "Advanced Lab 4"
author: "Derek Van Berkel"
date: '2022-11-17'
output: html_document
---


## Sentiment analysis
In this lab we are going to learn how we might be able to mine the opinions or sentiment of text. When reading a text, we use our contextual understanding and the emotional intent of words to infer whether a section of text is positive or negative. We are also able characterize additional emotions like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically. 

One way to analyze sentiment is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn’t the only way to approach sentiment analysis, but it is an often-used approach. We will be using the ```library(tidytext)``` package for this analysis. 

### The sentiments dataset
```tidytext``` is basically different dictionaries for evaluating the opinion or emotion in text. They include three general-purpose lexicons:

 * ```afinn``` from [Finn Årup Nielsen](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010),
 * ```bing``` from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and
 * ```nrc``` from [Saif Mohammad and Peter Turney](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm).

All three of these lexicons are based on unigrams, i.e., single English words that are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon is a binary categorization (“yes”/“no”) over categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words into positive and negative categories. The AFINN lexicon assigns words with a score between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. All of this information is tabulated in the sentiments dataset, that can be called using get_sentiments() using ```"afinn"```, ```"bing"``` and ```"nrc"```.

```{r}
library(tidytext)
library(textdata)
library(tidyverse)
get_sentiments("afinn")
```
### bing
```{r}
get_sentiments("bing")
```
### nrc
```{r}
get_sentiments("nrc")
```
How do we know that sentiment matches these lexicons? They were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. For this reason it not a good idea to apply them to text that is dramatically different from their validation datasets (e.g. narrative fiction). While it is true that using these sentiment lexicons may give us less accurate results for Jane Austin compared to tweets sent by a contemporary writer, we still can measure the sentiment content for words that are shared across the lexicon and the text.

Not every word is in the lexicons, because many words are neutral. It is also important to keep in mind that these methods cannot detect qualifiers before a word, such as in “no good” or “not true”; as it is based unigrams/single word only. Keep in mind whether your text includes instances of sarcasm or negated text when doing your sentiment analysis.

The size of the chunk of text that we use can also influence sentiment scores. A text the size of many paragraphs can often have positive and negative sentiment that average out to zero, while sentence-sized or paragraph-sized text often works better.


### Sentiment analysis using inner join
We are using the tidy format which means we can use inner join to run a sentiment analysis. ```inner_join(x, y)```: Return all rows from ```x``` where there are matching values in ```y```, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned. This is a mutating join.

We will grab the lyrics from the album "When We All Fall Asleep, Where Do We Go?" by Billie Eilish to analyze sentiment. Download the project file from Canvas.  

```{r}
wheredowego <- read.csv("wheredowego.csv")
```

Now let’s look at the words with an anger score from the NRC lexicon. With this procedure we can answer what are the most common negative words in bad guy? We will first grab a word with a negative connotation 

```{r}
nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")
nrc_anger
```

We now convert the text to the tidy format using ```unnest_tokens()```. By using this function, we split each row so that there is one token (word) in each row a new data frame. By default tokenization is for single words, as shown here. This also retains other columns (e.g. line number), strips punctuation and converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior). We use group_by and mutate to construct columns that indicate line and words in that line.
 
```{r} 
library(stringr)
## we need to make sure that the lyrics are characters
wheredowego$lyric <- as.character(wheredowego$lyric)

tidy_song <- wheredowego %>%
  group_by(track_title) %>%
  ungroup() %>%
  unnest_tokens(word,lyric)
```

Now we use inner_join to connect the nrc lexicon with those of the song. We also add the count() function to sum the number of negative words 

```{r}
tidy_song %>%
  filter(track_title == "bad guy")%>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE)
```  

Now let's assess the sentiment of the different song lines in the song of the entire album. First we use ```inner_join``` to compare the lexicon with the lyrics. We count the words classified as positive and negative per line. We use the spread function to organize negative and positive columns per line (some line have no words in the lexicon). We apply a zero to the cases without a negative or positive word. Finally, we create a new variable that calculates the total sentiment (positive count minus negative).   
  

```{r}
song_sentiment <- tidy_song %>%
  inner_join(get_sentiments("bing")) %>%
  count(track_title, index = line, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```
Now we can plot the results to visualize the sentiment of the tracks for the entire album

```{r}
ggplot(song_sentiment, aes(index, sentiment, fill = track_title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~track_title)
```


## What sentiment dictionary should I use?
Which sentiment lexicon is the most appropriate for your purposes. We can explore this by examining difference between sentiment evaluation for the different lexicons. We will start by filtering (```filter()```) only the lyrics from the song bad guy.


Let’s again use integer division (%/%) to define larger sections of text that span multiple lines, and we can use the same pattern with count(), spread(), and mutate() to find the net sentiment in each of these sections of text.

```{r}
afinn <- tidy_song %>% 
  filter(track_title == "bad guy") %>%
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = line) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(tidy_song %>% 
                            filter(track_title == "bad guy") %>%
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          tidy_song %>% 
                            filter(track_title == "bad guy") %>%
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method, index = line, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

We can now visualize the estimates of the net sentiment (positive - negative) for the song across each sentiment lexicon. Here we use the ```facet_wrap```, which wraps a 1d sequence of panels into 2d, efficiently displaying multiple charts on the screen. You can find more information on facet_wrap [here](https://plot.ly/ggplot2/facet_wrap/)  

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

There are differences in the result of the three different sentiment lexicons, but they have similar relative trajectories. We see similar positive and negative dips and peaks. To investigate these difference can look at the difference between positive and negative words across these lexicons.

```{r}
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)
```
Both lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon. 

### Evaluating common positive and negative words
We can analyze word counts that contribute to each sentiment using  ```count()```, and find out how much each word contributed to each sentiment.

```{r}
bing_word_counts <- tidy_song %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

We can also visualize these difference using ggplot2. 
```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

## Word clouds
There are different ways to visualize difference in sentiment. Here we will use a word cloud. Word clouds a visual representation of the words used in a particular piece of text, with the size of each word indicating its relative frequency. Here we will visualize the lyrics of the entire album. 

```{r}
library(wordcloud)
library(RColorBrewer)
tidy_song %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

We might want to visualize the sentiment of the album. For this we can use ```comparison.cloud()```. This requires converting the data frame into a matrix with reshape2’s acast(). acast is similar to spread as it helps to reorganize columns based on variable. In this case we are taking counts of negative and positive values and transposing them to individual columns into a matrix. We then call the comparison.cloud function on this matrix.  


```{r}
library(reshape2)

tidy_song %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

## Contextaulizing sentiment beyond just words
While analyzing sentiment at the word level can be helpful, a deeper examination of different text units is often required to grasp the true conveyed meanings. Sentiment analysis solely based on unigrams (single words) might overlook the actual sentiment of an entire sentence. For instance, consider the sentence, "I am not having a good month," where the adverb "not" reverses an otherwise positive statement to a negative one.

To gain a more comprehensive understanding of sentence-level sentiment, one option is to utilize more sophisticated algorithms. A valuable tool for this purpose in R is the `sentimentr` package, designed to efficiently assess text polarity sentiment at the sentence level and, if needed, aggregate the results by rows or grouping variables. This package has been employed in academic research and simplifies sentiment analysis, making it attainable with just a few lines of code.


```{r}
library(sentimentr)

song_sentiment_sent <- wheredowego %>%
    get_sentences() %>%
    sentiment_by(by = c('track_title', 'line'))%>%
  as.data.frame()
```


Now we can plot the results to visualize the sentiment of the tracks for the entire album

```{r}
ggplot(song_sentiment_sent, aes(line, ave_sentiment, fill = track_title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~track_title)
```


```{r}
song_emotion <- wheredowego %>%
  filter(track_title == "bad guy")%>% 
  get_sentences() %>%
    emotion_by(by = c('track_title', 'line'))%>%
  as.data.frame()
```

Numerous online sources offer access to full-text books, documents, and various media. For instance, I have discovered data on Kaggle and utilized various APIs to acquire relevant information.

#### Genius 
```{r}
#install.packages("geniusr", include = FALSE)
#library(geniusr)
#search_artist("Bob Dylan", n_results = 10)
###the simplest way to get lyrics is to use the url of the song on Genius
#get_lyrics_url(song_lyrics_url = "https://genius.com/Bob-dylan-visions-of-johanna-lyrics")

```



An alternative is to use spotify to grab the lyrics <https://www.rcharlie.com/spotifyr/>. I have not tested this package, but it looks good. You are welcome to use this, but I prefer not to demonstrate it as spotify supports some questionable narratives about the environment and social issues. 



## Assignment
### Questions
1. Utilize sentiment analysis to study a textual document in a manner you find suitable. Select a lexicon library for assessing the sentiment of the dataset, such as determining whether it is positive, negative, or joyous. Present your findings using appropriate charts and provide an explanation of the results in 1-2 paragraphs.

2. Conduct an analysis of ambiguous text within the dataset. Reflect on and provide examples of issues related to subjectivity, tone, context, polarity, irony, sarcasm, comparisons, and the use of neutral language in 2-3 paragraphs.

3. Our readings for this week encompass an application of sentiment analysis and a discussion on the ethics of utilizing crowd-sourced data in research. Offer a critical evaluation of the ethics, research design, and conclusions drawn in these readings in 2-3 paragraphs.


